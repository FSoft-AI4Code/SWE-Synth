model_name_or_path: Qwen/Qwen2.5-Coder-14B-Instruct
run_name: qwen-sft-14B_swesynth-same_instances_comparison_cap1
output_dir: saves/qwen-sft-14B_swesynth-same_instances_comparison_cap1
tokenized_path: saves/qwen-sft-14B_swesynth-same_instances_comparison_cap1/tokenized_data
dataset: swesynth-same_instances_comparison_cap1

template: qwen
cutoff_len: 65536
# max_samples: 1000
overwrite_cache: true

# num_gpus x per_device_train_batch_size x gradient_accumulation_steps = global_batch_size
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 2.0e-5
num_train_epochs: 5.0

# ---
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
shift_attn: true
flash_attn: fa2
neat_packing: true
use_unsloth_gc: true
preprocessing_num_workers: 16

logging_steps: 10
save_steps: 30
plot_loss: true
overwrite_output_dir: true

lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
enable_liger_kernel: true
report_to: wandb
